{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3nl50l3lkvbihsh11skv3",
    "id": "Dheykc-SdqsK"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellId": "9r5cibancpvjx1jjy6kl",
    "id": "sB7ElaTEdtHt",
    "outputId": "95188dd1-6f9e-411b-a660-add29ba61552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 1.4 MB/s            \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.8 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-22.0.4\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.14.0)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /kernel/fallback/lib/python3.8/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.50.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyyaml==5.4.1\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyyaml\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 2.10 which is incompatible.\n",
      "dvc 1.4.0 requires PyYAML<5.4,>=5.1.2, but you have pyyaml 5.4.1 which is incompatible.\n",
      "cfn-lint 0.33.2 requires jsonschema~=3.0, but you have jsonschema 4.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyyaml-5.4.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /kernel/lib/python3.8/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.50.0)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /kernel/lib/python3.8/site-packages (from gdown) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /kernel/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14763 sha256=a20652628c17798109f1324dd943f322889c75e809e1d225b047579df80b010c\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/7b/7b/5d/656f46cd6889e4c93977be9586901d0adc1271b2d876c84c96\n",
      "Successfully built gdown\n",
      "Installing collected packages: PySocks, gdown\n",
      "\u001b[33m  WARNING: The script gdown is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed PySocks-1.7.1 gdown-4.4.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.24)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /kernel/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /kernel/lib/python3.8/site-packages (from wandb) (51.0.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (5.7.3)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /kernel/lib/python3.8/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: PyYAML in /home/jupyter/.local/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=b78ebe0c2bc54eb115edda7cd5e1ed4a044b0c4e62d3f2f2cd22cc22fce14087\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 wandb-0.12.16\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install --upgrade pip\n",
    "%pip install transformers\n",
    "%pip install pyyaml==5.4.1\n",
    "%pip install gdown\n",
    "%pip install wandb\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "q85y8wh0otf27py112dgu1",
    "id": "iVYS3gXnDiiE"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellId": "s349gporhr8o79p8it9zf",
    "id": "LzcgBFlu7_MD",
    "outputId": "013c8684-e04c-4070-f265-79a00cb64170"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellId": "zu7efb3zgjyrikocsn5c",
    "id": "TqqKTiuwxw1e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "MODEL_TYPE = \"microsoft/deberta-v3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellId": "fzb79smp0ulzfffr3ohj78",
    "id": "HTx3wjD4SmON"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellId": "wcgqrqrh7og0lor8cj89e6",
    "id": "RSxGejvYy9Yo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "tgt_vocab_size = 54 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cellId": "xxp5lwfahjrcdc27xxo7pi",
    "id": "Bu8UjSoVo-R_"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "g7nla6nuzcok8haqhg7me",
    "id": "rI6qF9jADOI-"
   },
   "source": [
    "# Download AAPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellId": "xdfbyhyl8dj6wv5z3gj65t",
    "id": "6u3S-OIEdhX9",
    "outputId": "5325390a-0562-42fa-c25b-290f64e31e43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/work/resources/label_test',\n",
       " '/home/jupyter/work/resources/label_train',\n",
       " '/home/jupyter/work/resources/label_val',\n",
       " '/home/jupyter/work/resources/test.tsv',\n",
       " '/home/jupyter/work/resources/text_test',\n",
       " '/home/jupyter/work/resources/text_train',\n",
       " '/home/jupyter/work/resources/text_val',\n",
       " '/home/jupyter/work/resources/train.tsv',\n",
       " '/home/jupyter/work/resources/validation.tsv']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/drive/folders/1qw05BnA1O-XDgJ50OgNGFSlTa9Kls00j?usp=sharing'\n",
    "gdown.download_folder(url, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "9jjkf78mp1s2k14rnof1ps",
    "execution": {
     "iopub.execute_input": "2022-05-09T14:16:30.707582Z",
     "iopub.status.busy": "2022-05-09T14:16:30.707326Z",
     "iopub.status.idle": "2022-05-09T14:16:30.711535Z",
     "shell.execute_reply": "2022-05-09T14:16:30.710504Z",
     "shell.execute_reply.started": "2022-05-09T14:16:30.707548Z"
    },
    "id": "ls_txy4OrK6U"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# !cp -r drive/MyDrive/AAPD ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellId": "kq4nuuotrzlgkwokv7rcrr",
    "id": "tsqss4vgkL08"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# !mkdir AAPD\n",
    "# !mv *.tsv AAPD\n",
    "# !mv text_* AAPD\n",
    "# !mv label_* AAPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zdae0e5b5kxv77ktspm",
    "id": "BdeqBysrSO6o"
   },
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "cellId": "yk24u9gzw19te4iulvhq7",
    "id": "rGJnw0adHdzI"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def apply_to_dict_values(dict, f):\n",
    "    for key, value in dict.items():\n",
    "        dict[key] = f(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "cellId": "11frj4o4ohmet5qhjzsyde",
    "id": "1ZvjszV0S5KG"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class AAPDDataset(Dataset):\n",
    "    \"\"\"AAPD dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(self.path, sep='\\t', header=None)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def target_to_tensor(target):\n",
    "        return torch.tensor([float(label) for label in target])\n",
    "\n",
    "    @staticmethod\n",
    "    def target_to_tensor_with_specials(target):\n",
    "        return torch.tensor([BOS] + [float(index) + 4 for index, label in enumerate(target) if label == '1'] + [EOS])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.tokenizer(self.data.iloc[idx, 1], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True) # max_len=512 !DocBERT\n",
    "        apply_to_dict_values(data, lambda x: x.flatten())\n",
    "        return data, AAPDDataset.target_to_tensor_with_specials(self.data.iloc[idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "cellId": "k5qga7dhtq8impxafbedg",
    "id": "stCFG3BpsjVY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f91e7ef47447cd874240149aac6e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7642cc1af3ab47dfa06000c2040b0e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2464616.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_dataset = AAPDDataset('./AAPD/train.tsv')\n",
    "val_dataset = AAPDDataset('./AAPD/validation.tsv')\n",
    "test_dataset = AAPDDataset('./AAPD/test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "cellId": "eujc5kom8acuqt8itc7lnf",
    "id": "sXRJMMUf0YRj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def padding(data):\n",
    "    src, tgt = zip(*data)\n",
    "\n",
    "    keys = src[0].keys()\n",
    "    src_agg = {}\n",
    "    for key in keys:\n",
    "        agg = [s[key] for s in src]\n",
    "        src_agg[key] = torch.stack(agg)    \n",
    "\n",
    "    tgt_len = [len(t) for t in tgt]\n",
    "    tgt_pad = torch.zeros(len(tgt), max(tgt_len)).long()\n",
    "    for i, s in enumerate(tgt):\n",
    "        tgt_pad[i, :tgt_len[i]] = s.detach().clone()[:tgt_len[i]]\n",
    "\n",
    "    return src_agg, tgt_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "cellId": "h05vcrs0ee9fqdfbcba2w",
    "id": "U-good4bDmy2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=padding)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=padding)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5uyi0ou74dwvm326q126y",
    "id": "PEdFRi6TeFnY"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fdc1wzk7l37lk7ejfr6zjp",
    "id": "5QGasR8jCs1E"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "cellId": "hqlcnz1i7hh7b3d8rfxzo",
    "id": "VFcwlkmSCoMx"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DebertaEncdoer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DebertaEncdoer, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(MODEL_TYPE)\n",
    "        self.hidden_size = 1024\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Bert input -> hidden states for SGM attention, (hidden state, cell state) for decoder init \n",
    "        [batch_size, seq_len] -> [batch_size, seq_len, encoder_hidden_size], ([1, batch_size, encoder_hidden_size] x 2)\n",
    "        1 = n_decoder_layers\n",
    "        '''\n",
    "        batch_size = inputs['input_ids'].size(0)\n",
    "        output = self.model(**inputs)\n",
    "        return output.last_hidden_state, (output.last_hidden_state.transpose(0, 1)[:1], output.last_hidden_state.transpose(0, 1)[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sigdod77ipcsejguyqcb5p",
    "id": "WrTbB4-VnHFW"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellId": "lph13nx7jgxhdkftbupmk",
    "id": "VKdO7lELsXFs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class SgmAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size, att_hidden_size):\n",
    "        super(SgmAttention, self).__init__()\n",
    "        self.U = nn.Linear(encoder_hidden_size, att_hidden_size)\n",
    "        self.W = nn.Linear(decoder_hidden_size, att_hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.V = nn.Linear(att_hidden_size, 1) # seq_len x att_sz -> seq_len\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def init_context(self, context):\n",
    "        '''\n",
    "        Context from encoder. Size: [batch_size, seq_len, encoder_hidden_size]\n",
    "        '''\n",
    "        self.context = context\n",
    "\n",
    "\n",
    "    def forward(self, s):\n",
    "        state_term = self.W(s).unsqueeze(1) # batch_size x decoder_hidden_size -> batch_size x 1 x att_hidden_size\n",
    "        context_term = self.U(self.context) # batch_size x seq_len x encoder_hidden_size -> batch_size x seq_len x att_hidden_size\n",
    "        sum_activation = self.tanh(context_term + state_term.expand_as(context_term)) # batch_size x seq_len x att_hidden_size\n",
    "        weights = self.V(sum_activation).squeeze(-1) # batch_size x seq_len\n",
    "        softmax_weights = self.softmax(weights)\n",
    "        c_t = torch.bmm(softmax_weights.unsqueeze(1), self.context).squeeze(1) # batch_size x seq_len\n",
    "        # output = self.linear_out(torch.cat([h, c_t], 1))\n",
    "\n",
    "        return c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "bb9c9tqnqzucrkzgrdz1",
    "id": "-hKgG4uQnKqq"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellId": "zrcthzrtvqfvm84p4cudfh",
    "id": "vj_6mkBmqXK8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, num_layers, input_size, hidden_size, dropout):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.LSTMCell(input_size, hidden_size))\n",
    "            input_size = hidden_size\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        h_0, c_0 = hidden\n",
    "        h_1, c_1 = [], []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h_1_i, c_1_i = layer(input, (h_0[i], c_0[i]))\n",
    "            input = h_1_i\n",
    "            if i + 1 != self.num_layers:\n",
    "                input = self.dropout(input)\n",
    "            h_1 += [h_1_i]\n",
    "            c_1 += [c_1_i]\n",
    "\n",
    "        h_1 = torch.stack(h_1)\n",
    "        c_1 = torch.stack(c_1)\n",
    "\n",
    "        return input, (h_1, c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellId": "gsa74cakulay8nr0yfam4m",
    "id": "CsOeunVqEWiz"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class RnnDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, tgt_vocab_size, hidden_size, input_size, encoder_hidden_size):\n",
    "        super(RnnDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.input_size = input_size\n",
    "        dropout_prob = 0.2\n",
    "        num_layers=1\n",
    "\n",
    "        self.rnn = StackedLSTM(input_size=self.input_size, \n",
    "                               hidden_size=self.hidden_size,\n",
    "                               num_layers=num_layers, \n",
    "                               dropout=dropout_prob)\n",
    "\n",
    "        self.inner_hidden_size = 768\n",
    "        self.W_d = nn.Linear(self.hidden_size, self.inner_hidden_size)\n",
    "        self.V_d = nn.Linear(self.encoder_hidden_size, self.inner_hidden_size)\n",
    "        self.W_o = nn.Linear(self.inner_hidden_size, tgt_vocab_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input, state, c_t, prev_predicted_labels=None): \n",
    "        output, state = self.rnn(input, state)\n",
    "        return output, state\n",
    "\n",
    "    def compute_score(self, hiddens, c_t, prev_predicted_labels=None, use_softmax=False):\n",
    "        scores = self.W_o(self.activation(self.W_d(hiddens) + self.V_d(c_t)))\n",
    "        I = torch.zeros_like(scores)\n",
    "        if prev_predicted_labels:\n",
    "            for predicted_labels in prev_predicted_labels:\n",
    "                I[(list(range(I.size(0))), predicted_labels)] = -1 * float('inf')\n",
    "        scores = scores + I\n",
    "        if use_softmax:\n",
    "            scores = self.softmax(scores)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "dcvcy3q99h4dfa58ombajf",
    "id": "o8ICv8HxeWy_"
   },
   "source": [
    "# BERT + SGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "cellId": "0m71r00ep6srou7i02q1cfj",
    "id": "bWmFilIWeYoI"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class BertSGM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertSGM, self).__init__()\n",
    "        tgt_vocab_size = 58\n",
    "        tgt_embedding_size = 768\n",
    "        encoder_hidden_size = 1024\n",
    "        self.decoder_hidden_size = 768\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, tgt_embedding_size)\n",
    "        self.encoder = DebertaEncdoer()\n",
    "        self.mapping = nn.Linear(encoder_hidden_size, self.decoder_hidden_size)\n",
    "        self.attention = SgmAttention(encoder_hidden_size=encoder_hidden_size, decoder_hidden_size=self.decoder_hidden_size, att_hidden_size=768) \n",
    "        self.decoder = RnnDecoder(tgt_vocab_size=tgt_vocab_size, hidden_size=self.decoder_hidden_size, input_size=encoder_hidden_size + tgt_embedding_size, encoder_hidden_size=encoder_hidden_size)\n",
    "        self.criterion = self.create_criterion(tgt_vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        context, encoder_state = self.encoder(src)\n",
    "        \n",
    "        self.attention.init_context(context)\n",
    "        y_hats = self.tgt_embedding(tgt[:, :-1])\n",
    "\n",
    "        batch_size = y_hats.size(0)\n",
    "        prev_predicted_labels = []\n",
    "        saved_scores = []\n",
    "        decoder_state = (self.mapping(encoder_state[0]), self.mapping(encoder_state[1]))\n",
    "\n",
    "        for y_hat, t in zip(y_hats.split(1, dim=1), tgt[:, 1:].transpose(0, 1)):\n",
    "            c_t = self.attention(decoder_state[0].squeeze(0))\n",
    "            input = torch.cat([y_hat.squeeze(1), c_t], dim=-1)\n",
    "            output, decoder_state = self.decoder(input, decoder_state, c_t)\n",
    "            scores = self.decoder.compute_score(output, c_t, prev_predicted_labels)\n",
    "            saved_scores.append(scores)\n",
    "            prev_predicted_labels.append(t)\n",
    "        \n",
    "        scores = torch.stack(saved_scores).transpose(0, 1)\n",
    "        return self.compute_loss(scores, tgt)\n",
    "    \n",
    "    def compute_loss(self, scores, tgt):\n",
    "        loss = 0.\n",
    "        for score, t in zip(scores, tgt[:, 1:]):\n",
    "            loss += self.criterion(score, t)\n",
    "        return loss / tgt.size(0)\n",
    "    \n",
    "    def create_criterion(self, tgt_vocab_size):\n",
    "        weight = torch.ones(tgt_vocab_size)\n",
    "        weight[PAD] = 0\n",
    "        crit = nn.CrossEntropyLoss(weight, ignore_index=PAD)\n",
    "        return crit\n",
    "    \n",
    "    def predict(self, src, max_steps=10):\n",
    "        context, encoder_state = self.encoder(src)\n",
    "        self.attention.init_context(context)\n",
    "        batch_size = src['input_ids'].size(0)\n",
    "        y_hat = self.tgt_embedding(torch.tensor([BOS for _ in range(batch_size)]).to(device))\n",
    "        decoder_state = (self.mapping(encoder_state[0]), self.mapping(encoder_state[1]))        \n",
    "        \n",
    "        predicted_labels = []\n",
    "        eos_predicted = torch.tensor([False for _ in range(batch_size)]).to(device)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            c_t = self.attention(decoder_state[0].squeeze(0))\n",
    "            input = torch.cat([y_hat.squeeze(1), c_t], dim=-1)\n",
    "            output, decoder_state = self.decoder(input, decoder_state, c_t)\n",
    "            scores = self.decoder.compute_score(output, c_t, predicted_labels)\n",
    "            prediction = torch.argmax(scores, dim=-1)\n",
    "            y_hat = self.tgt_embedding(prediction.to(device))\n",
    "            predicted_labels.append(prediction.tolist())\n",
    "            eos_predicted = eos_predicted | (prediction == EOS)\n",
    "            if torch.all(eos_predicted):\n",
    "                break\n",
    "\n",
    "        return torch.tensor(predicted_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2dzb42o3r5kszmx7t4iao8",
    "id": "3lyKPv3GptgH"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellId": "bvmve3srg8ywksitca57q",
    "id": "4jXyu5SSpwbR"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def one_hot_labels(batch, n_classes=54, n_specials=4):\n",
    "    batch_labels = []\n",
    "    for tensor in batch:\n",
    "        labels = [0 for _ in range(n_classes)]\n",
    "        for elem in tensor:\n",
    "            if elem == EOS:\n",
    "                break\n",
    "            if elem >= n_specials:\n",
    "                labels[elem - n_specials] = 1\n",
    "        batch_labels.append(labels)\n",
    "    return batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellId": "5vgyirgcvdodcif0syz0v",
    "id": "jCWW4Vjesg-8",
    "outputId": "c204cad7-b936-48ab-f9ee-76ea0436216c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-09 18:12:07--  https://gist.githubusercontent.com/ArseniyBolotin/7623835da1631b00fb150bcd5b0d909f/raw/wandb_writer.py\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2609 (2.5K) [text/plain]\n",
      "Saving to: ‘wandb_writer.py’\n",
      "\n",
      "wandb_writer.py     100%[===================>]   2.55K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-05-09 18:12:08 (53.2 MB/s) - ‘wandb_writer.py’ saved [2609/2609]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!wget https://gist.githubusercontent.com/ArseniyBolotin/7623835da1631b00fb150bcd5b0d909f/raw/wandb_writer.py -O wandb_writer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellId": "6cq3vljv4ld9lyf9l8aoa8",
    "id": "1ApIPmpXsP-9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn import metrics\n",
    "from wandb_writer import WandbWriter\n",
    "\n",
    "def get_metrics(y, y_pre):\n",
    "        hamming_loss = metrics.hamming_loss(y, y_pre)\n",
    "        macro_f1 = metrics.f1_score(y, y_pre, average=\"macro\")\n",
    "        macro_precision = metrics.precision_score(y, y_pre, average=\"macro\")\n",
    "        macro_recall = metrics.recall_score(y, y_pre, average=\"macro\")\n",
    "        micro_f1 = metrics.f1_score(y, y_pre, average=\"micro\")\n",
    "        micro_precision = metrics.precision_score(y, y_pre, average=\"micro\")\n",
    "        micro_recall = metrics.recall_score(y, y_pre, average=\"micro\")\n",
    "        \n",
    "        return {\n",
    "            \"hamming_loss\": hamming_loss,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"macro_precision\": macro_precision,\n",
    "            \"macro_recall\": macro_recall,\n",
    "            \"micro_f1\": micro_f1,\n",
    "            \"micro_precision\": micro_precision,\n",
    "            \"micro_recall\": micro_recall\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vhvnnebs8uhv1udb2tcvl",
    "id": "uKCsZUUdtbXG"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cellId": "9e6f5k15yaqm8l65fo092"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b4a0cdcbb46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "del model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "cellId": "67u9fjr9ezo2cdvg3eibsm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda memory allocated: 1.673 Gb\n",
      "Cuda memory allocated: 1.673 Gb\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import gc\n",
    "\n",
    "# torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats('cuda')\n",
    "print(\"Cuda memory allocated: {:.4} Gb\".format(torch.cuda.max_memory_allocated('cuda') / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "cellId": "i17vkvla3fzivgg6zi18r",
    "id": "sdKnwg3TcfvZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb47409e26bc46cb944671f30d1e7e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=580.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1426f9f0d1b43fc802a19be65143cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=873673253.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = BertSGM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cellId": "nimdxsk5ube2903nh9qvq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# from torch.utils.checkpoint import checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cellId": "ru3mzpz0v3vilvkyszkq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# model.encoder.model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "cellId": "j8x1nqm793b2yhw4wds8cp",
    "id": "QsMCR79ab-mK"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=2e-5, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "cellId": "jkjt89xurrpati9n4hnlr",
    "id": "F6EgYtYPt7ny"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc3n34ka\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/work/resources/wandb/run-20220511_041910-tnts22fn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/c3n34ka/BERT%2BSGM%20experiment/runs/tnts22fn\" target=\"_blank\">northern-armadillo-20</a></strong> to <a href=\"https://wandb.ai/c3n34ka/BERT%2BSGM%20experiment\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "wb_writer = WandbWriter(\"BERT+SGM experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "lpi6trfx5y43qf878qiim",
    "id": "6hgnA0cOegY-"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellId": "evdmvmzf206tthtpvrykh",
    "id": "gcflncwvuknu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cellId": "tdm9qz5n6wdycc26rof7m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = torch.load('deberta_sgm_5.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "cellId": "ws5pausehjdv114s55c2",
    "id": "mk-XVD8VtkuT"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train_epoch(model, optimizer,  dataloader, val_dataloader, val_freq, wb_writer=None):\n",
    "    model.train()\n",
    "    index = 0\n",
    "    for src, tgt in tqdm(dataloader, leave=False):\n",
    "        index += 1\n",
    "        apply_to_dict_values(src, lambda x: x.to(device))\n",
    "        tgt = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(src, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if wb_writer:\n",
    "            wb_writer.add_scalar(\"Batch train loss\", loss.item())\n",
    "            wb_writer.next_step()\n",
    "            wb_writer.add_scalar(\"Step\", wb_writer.step)\n",
    "        if index % val_freq == 0:\n",
    "            eval_model(model, val_dataloader, wb_writer, '_validation')\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5x7wbh3wmqgj3c131ir1q",
    "id": "ilLfCn6Aj7T5"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellId": "u0umye325zloz03jskq9q",
    "id": "Q45wES4Uug-g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def eval_model(model, dataloader, wb_writer, suffix):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            apply_to_dict_values(src, lambda x: x.to(device))\n",
    "            tgt = tgt.to(device)\n",
    "            prediction = model.predict(src)\n",
    "            targets.extend(tgt.tolist())\n",
    "            predictions.extend(prediction.t().tolist())\n",
    "    \n",
    "    results = get_metrics(one_hot_labels(targets), one_hot_labels(predictions))\n",
    "\n",
    "    if wb_writer:\n",
    "        for k, v in results.items():\n",
    "            name = k\n",
    "            if suffix:\n",
    "                name += suffix\n",
    "            wb_writer.add_scalar(name, v)\n",
    "        wb_writer.next_step()\n",
    "        wb_writer.add_scalar(\"Step\", wb_writer.step)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0ooy3nqb5kcmzay456a89a",
    "id": "9hAhxhsc-QSF"
   },
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "koyrh8r7lknpg3lh5vy779",
    "id": "rbhlcAX0xWlC",
    "outputId": "fcbbfec7-e045-4fa8-c9f1-c540e9e29091"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_epoch(model, optimizer, train_dataloader, val_dataloader, 100, wb_writer)\n",
    "#     log = eval_model(model, train_dataloader, wb_writer, '_train')\n",
    "#     print(log)\n",
    "    log = eval_model(model, val_dataloader, wb_writer, '_validation')\n",
    "    print(log)\n",
    "    log = eval_model(model, test_dataloader, wb_writer, '_test')\n",
    "    print(log)\n",
    "    torch.save(model, 'deberta_sgm_hidden_' + str(epoch) + '.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "cellId": "k6k3agjbimj58uc79e66",
    "id": "Iv3uUuRRBQE3",
    "outputId": "be44564f-b8e4-4c1f-cb45-fa0f54961da8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'deberta_sgm_1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d381e9a1efe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'deberta_sgm_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'deberta_sgm_1.pt'"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for epoch in range(1, 5 + 1):\n",
    "    model = torch.load('deberta_sgm_hidden_' + str(epoch) + '.pt')\n",
    "    print(epoch)\n",
    "    print(eval_model(model, val_dataloader, None, '_validation'))\n",
    "    print(eval_model(model, test_dataloader, None, '_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "cellId": "ab6a7a19uh0b03z8v37v5e",
    "id": "ZtcQCk1IRfjo"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb_writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc3a08b4813d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb_writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb_writer' is not defined"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "wandb_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dtlzu3h827w39u9yh5jb0g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "33571d20-8ad0-4938-bce0-bb986e27d853",
  "notebookPath": "deberta-sgm.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
